\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\DeclareGraphicsExtensions{.pdf,.jpg,.png,.jpeg}
\graphicspath{{images/}, {figs/}}
\newcommand{\todo}[1]{\textcolor{red}{{\em [#1]}} }
\newcommand{\specialcell}[2][c]{%
    \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\equref}[1]{Equation~\ref{equ:#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Survey On Image and Video Captioning}

\author{Weilian Song\\
University of Kentucky\\
{\tt\small weilian.song@uky.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% BODY TEXT
\section{Introduction}

Automatic image and video captioning is a relatively new field in the field of
Machine Learning, where a program can automatically assign a
sentenced-description of an image, or in the case of a video, for every frame
of the video. Desite being new, this technology has improved itself over the
past three years and is widely used in today's world, from auto-generating
video captions for Youtube videos to clustering images/videos based on their
captions.

In this survey, we first explore the basic concepts of a nerual image/video
caption generator, which include topics like Recurrent Neural Networks (RNN),
Long-Short-Term-Memory (LSTM), and a naive model for performing such task.
\todo{cite?} We will also explain various evaluation methods that authors use
to compare their models with others. \todo{figure out their eval methods}

Next we explore two approaches for improving the accuracy of the naive method,
one through the use of an attention mechanism, and the other through the
process of dense labelling. Both of these methods are imporved upon in this
past year's CVPR conference, with new state-of-the-art results within their
respective field.

At the end, we will look at two papers that has nothing to do with the above
two approaches, but are very interesting in my opinion and help motivated my
interest in image and video captioning.

%-------------------------------------------------------------------------
\section{Background Information}

RNN

LSTM

Naive model for a captioner

Different evaluation methods, assuming I need to explain them

\section{Attention Mechanism}

Attention Mechanism

\subsection{Knowing When To Look Notes}

\begin{itemize}
  \item The network learns when to pay attention to images and when to pay
        attention to learned words. Uses gate $g_t$, which is learnable
  \item Novel attention mechanism for visual learning, inspired by success in
        residual learning. Different from other methods, as LSTM's cell state
        is used for learning
  \item Extensive evaluation on the visuan sentinel and the attention mechanism
\end{itemize}

\subsection{Areas of Attention for Image Captioning}

\begin{itemize}
  \item Uses regions of image to predict the next word, and uses predicted
        regions in the next state of the RNN cell.
  \item Have three different types of regions of interest. Either grid, object
        bounding box, or spatial transformers.
  \item Uses VGG16
\end{itemize}

\section{Dense Labeling}

Dense labeling

\section{Miscellaneous}

Miscellaneous papers I want to talk about

\section{Conclusion}

Conclusion

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
